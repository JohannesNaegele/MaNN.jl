{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f(x) = 3*x^2\n",
    "ForwardDiff.derivative(f,3) # berechnet die Ableitung von f(x) an der Stelle 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [27, 81]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       "  18\n",
       " 108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function f!(y,x)\n",
    "    y[1] = 3*x^2\n",
    "    y[2] = x^4\n",
    "    nothing\n",
    "end\n",
    "\n",
    "y = [0,0]\n",
    "f!(y,3)\n",
    "@show y\n",
    "\n",
    "y = [0,0]\n",
    "ForwardDiff.derivative(f!, y, 3) #Syntax .derivative(funktion, Variable, Stelle der Ableitung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       "  6\n",
       " 32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g(x) = 3*x[1]^2+x[2]^4\n",
    "g([1,2])\n",
    "ForwardDiff.gradient(g,  [1,2])\n",
    "\n",
    "h(x,p) = p[1] *x[1]^2 +p[2]*x[2]^4\n",
    "h([1,2], [3,1])\n",
    "ForwardDiff.gradient(x -> h(x, [3,1]),[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ReverseDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       " 0\n",
       " 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g(x) = 3*x[1]^2+x[2]^4\n",
    "g([1,2])\n",
    "ReverseDiff.gradient(g,  [1,2])\n",
    "\n",
    "h(x,p) = p[1] *x[1]^2 +p[2]*x[2]^4\n",
    "h([1,2], [3,1])\n",
    "ReverseDiff.gradient(x -> h(x, [3,1]),[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 3.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Zygote\n",
    "f(x) = 3*x+5\n",
    "f(10), f'(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: LLVMExtra\n"
     ]
    }
   ],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `MNIST` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `MNIST` not defined\n",
      "\n",
      "Stacktrace:\n",
      "  [1] eval\n",
      "    @ .\\boot.jl:370 [inlined]\n",
      "  [2] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base .\\loading.jl:1903\n",
      "  [3] #invokelatest#2\n",
      "    @ .\\essentials.jl:819 [inlined]\n",
      "  [4] invokelatest\n",
      "    @ .\\essentials.jl:816 [inlined]\n",
      "  [5] (::VSCodeServer.var\"#202#203\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer c:\\Users\\Luisa\\.vscode\\extensions\\julialang.language-julia-1.54.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:19\n",
      "  [6] withpath(f::VSCodeServer.var\"#202#203\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer c:\\Users\\Luisa\\.vscode\\extensions\\julialang.language-julia-1.54.2\\scripts\\packages\\VSCodeServer\\src\\repl.jl:274\n",
      "  [7] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer c:\\Users\\Luisa\\.vscode\\extensions\\julialang.language-julia-1.54.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:13\n",
      "  [8] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC c:\\Users\\Luisa\\.vscode\\extensions\\julialang.language-julia-1.54.2\\scripts\\packages\\JSONRPC\\src\\typed.jl:67\n",
      "  [9] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer c:\\Users\\Luisa\\.vscode\\extensions\\julialang.language-julia-1.54.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:139\n",
      " [10] top-level scope\n",
      "    @ c:\\Users\\Luisa\\.vscode\\extensions\\julialang.language-julia-1.54.2\\scripts\\notebook\\notebook.jl:32"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux.Data.MNIST\n",
    "using Plots\n",
    "\n",
    "# Load MNIST data\n",
    "_, test_data = MNIST.traindata()\n",
    "\n",
    "# Load the trained model (assuming you've already trained a model)\n",
    "model = load(\"mnist_model.bson\")  # Replace with your model file name\n",
    "\n",
    "# Define a function to get misclassified examples\n",
    "function misclassified_examples(model, data)\n",
    "    misclassified = []\n",
    "    for (x, y) in data\n",
    "        pred = Flux.onecold(model(x))\n",
    "        true_label = Flux.onecold(y)\n",
    "        if pred != true_label\n",
    "            push!(misclassified, (x, true_label, pred))\n",
    "        end\n",
    "    end\n",
    "    return misclassified\n",
    "end\n",
    "\n",
    "misclassified_data = misclassified_examples(model, test_data)\n",
    "\n",
    "# Visualize some of the misclassified examples\n",
    "num_examples_to_visualize = 10\n",
    "plot_array = []\n",
    "for (x, true_label, pred) in misclassified_data[1:num_examples_to_visualize]\n",
    "    push!(plot_array, heatmap(reshape(x, 28, 28), color=:grays, title=\"True: $true_label, Predicted: $pred\"))\n",
    "end\n",
    "\n",
    "plot(plot_array..., layout=(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x = 2.0: (3.5838531634528574,)\n"
     ]
    }
   ],
   "source": [
    "using ReverseDiff\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = x^2 + sin(x)\n",
    "\n",
    "# Define the input value\n",
    "x = 2.0\n",
    "\n",
    "# Compute the gradient\n",
    "gradient_result = gradient(f, x)\n",
    "\n",
    "println(\"Gradient at x = $x: $gradient_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x = 2.0: 3.5838531634528574\n"
     ]
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = x^2 + sin(x)\n",
    "\n",
    "# Define the input value\n",
    "x = 2.0\n",
    "\n",
    "# Compute the gradient\n",
    "gradient_result = ForwardDiff.derivative(f, x)\n",
    "\n",
    "println(\"Gradient at x = $x: $gradient_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVERSEDIFF FASTER THAN FORWARDDIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.175 μs (18 allocations: 16.28 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.076 ms (173 allocations: 8.44 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000-element Vector{Float64}:\n",
       " 1.4404907492776222\n",
       " 0.3641136609090274\n",
       " 0.6578963057858984\n",
       " 0.860395528110609\n",
       " 1.2044421778501537\n",
       " 1.2142689380070693\n",
       " 0.3290150644767269\n",
       " 1.278188466856067\n",
       " 1.7407254585747634\n",
       " 1.9545773140781482\n",
       " ⋮\n",
       " 0.5680727783008848\n",
       " 1.706572468315645\n",
       " 0.66622831666345\n",
       " 1.19005750345353\n",
       " 1.0382907953611165\n",
       " 1.0958346723238006\n",
       " 0.16621766339843091\n",
       " 0.5207973292796837\n",
       " 0.5442212367900865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ReverseDiff\n",
    "using BenchmarkTools\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = sum(x .^ 2)\n",
    "\n",
    "# Input values\n",
    "x = rand(1000)\n",
    "\n",
    "# Benchmark ReverseDiff (backward differentiation)\n",
    "@btime gradient(f, x)\n",
    "\n",
    "# Benchmark ForwardDiff (forward differentiation)\n",
    "using ForwardDiff\n",
    "@btime ForwardDiff.gradient(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "µs: Microsekunden 1µs = 1*10^-6 s\n",
    "ms: Millisekunden 1ms = 1*10^-3 s\n",
    "ReverseDiff: 7*10^-6s = 0,000007s\n",
    "ForwardDiff: 2,049*10^-3s = 0,002049s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.300 ns (0 allocations: 0 bytes)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  5.300 ns (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ReverseDiff\n",
    "using BenchmarkTools\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = sum(x .^ 2)\n",
    "\n",
    "# Input values\n",
    "x = [1,2,3,4,5]\n",
    "\n",
    "# Benchmark ReverseDiff (backward differentiation)\n",
    "ableitungsrev = gradient(f,x)\n",
    "@btime ableitungsrev\n",
    "\n",
    "\n",
    "# Benchmark ForwardDiff (forward differentiation)\n",
    "using ForwardDiff\n",
    "ableitungsforw = ForwardDiff.gradient(f, x)\n",
    "@btime ableitungsforw\n",
    " \n",
    "ableitungsrev == ableitungforw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1411200080598672"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f(x::Vector) = sin(x[1]/x[2])\n",
    "x = vcat(1.5,0.5)\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: objects of type Float64 are not callable\nMaybe you forgot to use an operator such as *, ^, %, / etc. ?",
     "output_type": "error",
     "traceback": [
      "MethodError: objects of type Float64 are not callable\n",
      "Maybe you forgot to use an operator such as *, ^, %, / etc. ?\n",
      "\n",
      "Stacktrace:\n",
      " [1] vector_mode_dual_eval!(f::Float64, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{Float64, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{Float64, Float64}, Float64, 2}}}, x::Vector{Float64})\n",
      "   @ ForwardDiff C:\\Users\\Luisa\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\apiutils.jl:24\n",
      " [2] vector_mode_gradient(f::Float64, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{Float64, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{Float64, Float64}, Float64, 2}}})\n",
      "   @ ForwardDiff C:\\Users\\Luisa\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\gradient.jl:89\n",
      " [3] gradient(f::Float64, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{Float64, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{Float64, Float64}, Float64, 2}}}, ::Val{true})\n",
      "   @ ForwardDiff C:\\Users\\Luisa\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\gradient.jl:19\n",
      " [4] gradient(f::Float64, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{Float64, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{Float64, Float64}, Float64, 2}}})\n",
      "   @ ForwardDiff C:\\Users\\Luisa\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\gradient.jl:17\n",
      " [5] gradient(f::Float64, x::Vector{Float64})\n",
      "   @ ForwardDiff C:\\Users\\Luisa\\.julia\\packages\\ForwardDiff\\PcZ48\\src\\gradient.jl:17\n",
      " [6] top-level scope\n",
      "   @ c:\\Users\\Luisa\\Documents\\Seminar Computational Statistics\\MaNN.jl\\src\\autodifferentiation.ipynb:2"
     ]
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "forwardmode = ForwardDiff.gradient(f(x),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVERSE MODE VS. FORWARD MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.244 μs (25 allocations: 1.94 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  863.158 ns (5 allocations: 784 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       "  0.7071067811865476\n",
       " 12.0\n",
       "  8.0\n",
       "  6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "f(x::Vector) = sin(x[1]) + prod(x[2:end]);  # returns a scalar\n",
    "x = vcat(pi/4, 2:4)\n",
    "f(x)\n",
    "\n",
    "using Zygote\n",
    "@btime reversemode = gradient(f,x)\n",
    "\n",
    "using ForwardDiff\n",
    "@btime forwardmode = ForwardDiff.gradient(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Fall ist reverse mode marginal langsamer als forward mode, denn \n",
    "Input-Dimension = 2\n",
    "Output-Dimension = 1\n",
    "Es lässt sich erst ein deutlich größerer Unterschied feststellen, wenn man eine komplexere Funktion nimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reversemode == forwardmode\n",
    "# ergibt false, da forwardmode ist ein Vektor und reversemode ist ein Tupel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.938 μs (25 allocations: 8.66 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  128.800 μs (39 allocations: 369.45 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200-element Vector{Float64}:\n",
       " 0.6897428282430618\n",
       " 2.4547265011722097e-89\n",
       " 2.9624250017012377e-90\n",
       " 2.0517947668614588e-89\n",
       " 1.1207899158386772e-88\n",
       " 4.186187577940991e-90\n",
       " 2.903385333185479e-90\n",
       " 1.2516275403209047e-89\n",
       " 4.353442887045468e-90\n",
       " 2.6330113309392543e-90\n",
       " ⋮\n",
       " 3.4492258051058264e-90\n",
       " 9.491555107190879e-90\n",
       " 1.0606667675090935e-89\n",
       " 5.317354348370391e-90\n",
       " 2.972852054540731e-90\n",
       " 3.071795504853622e-90\n",
       " 2.5381557352798786e-89\n",
       " 6.292523738436738e-90\n",
       " 4.864819594381441e-90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "f(x) = sum(x .^ 2)\n",
    "x = rand(200)\n",
    "\n",
    "using Zygote\n",
    "@btime reversemode = gradient(f,x)\n",
    "\n",
    "using ForwardDiff\n",
    "@btime forwardmode = ForwardDiff.gradient(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sieht man schon, dass bei deutlich mehr Daten der reverse mode schneller ist, als der forward mode.\n",
    "Und hier ist auch Input-Dimension=200 >> Ouput-Dimension=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001475 seconds (496 allocations: 2.431 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28.747069 seconds (10.40 M allocations: 696.440 MiB, 1.46% gc time, 99.60% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200×200 Matrix{Float64}:\n",
       " -0.784689  0.0          0.0          …  0.0          0.0\n",
       "  0.0       0.0          2.57242e-87     6.12671e-87  2.32851e-87\n",
       "  0.0       2.57242e-87  0.0             7.18441e-87  2.7305e-87\n",
       "  0.0       1.23169e-87  1.44433e-87     3.43994e-87  1.30738e-87\n",
       "  0.0       4.89912e-87  5.7449e-87      1.36826e-86  5.20018e-87\n",
       "  0.0       1.18848e-87  1.39365e-87  …  3.31925e-87  1.26151e-87\n",
       "  0.0       5.71095e-85  6.69688e-85     1.59499e-84  6.0619e-85\n",
       "  0.0       1.80537e-86  2.11704e-86     5.04214e-86  1.91631e-86\n",
       "  0.0       1.20455e-87  1.4125e-87      3.36414e-87  1.27857e-87\n",
       "  0.0       3.25753e-87  3.81991e-87     9.09784e-87  3.45772e-87\n",
       "  ⋮                                   ⋱               \n",
       "  0.0       1.22461e-87  1.43602e-87     3.42016e-87  1.29986e-87\n",
       "  0.0       2.01864e-87  2.36714e-87     5.63778e-87  2.14269e-87\n",
       "  0.0       9.66942e-87  1.13387e-86     2.70053e-86  1.02636e-86\n",
       "  0.0       4.87885e-87  5.72113e-87     1.36259e-86  5.17866e-87\n",
       "  0.0       1.26696e-87  1.48568e-87  …  3.53844e-87  1.34482e-87\n",
       "  0.0       1.74351e-87  2.0445e-87      4.86937e-87  1.85065e-87\n",
       "  0.0       1.43049e-87  1.67744e-87     3.99515e-87  1.51839e-87\n",
       "  0.0       6.12671e-87  7.18441e-87     0.0          6.5032e-87\n",
       "  0.0       2.32851e-87  2.7305e-87      6.5032e-87   0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using Zygote\n",
    "@time hessian(f,x)\n",
    "\n",
    "using ForwardDiff\n",
    "@time ForwardDiff.hessian(f,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTODIFF: BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTODIFF IN LAYERS\n",
    "Trainieren des neuronalen Netzes, indem die Gewichte verbessert werden. Das geschieht durch Anwendung des Gradientenverfahrens, denn es muss das Minimum der Funktion E gefunden werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [0,1,2,3,4,5,6,7,8,9]\n",
    "ouput = [0,1,2,3,4,5,6,7,8,9]\n",
    "gewichtsmatrix = randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MaNN\n",
    "# Struktur des neuronalen Netzes\n",
    "# implementiere jeweils 4 Gewichts- und 4 Biasmatrizen\n",
    "mutable struct NeuronalNetwork\n",
    "    W1::Matrix{Float64}\n",
    "    b1::Vector{Float64}\n",
    "    W2::Matrix{Float64}\n",
    "    b2::Vector{Float64}\n",
    "    W3::Matrix{Float64}\n",
    "    b3::Vector{Float64}\n",
    "    W4::Matrix{Float64}\n",
    "    b4::Vector{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize_network (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funktion, die die Gewichts- und Biasmatrizen in ihrer jeweiligen Größe mit zufälligen Werten initialisiert\n",
    "# W hat so viele Zeilen wie Neuronen im nächsten hidden layer sind und so viele Spalten wie die Anzahl der Inputs ist\n",
    "# b ist ein Vektor mit so vielen Zeilen wie Neuronen in dem nächsten hidden layer sind\n",
    "function initialize_network(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    W1 = randn(hidden_size1, input_size)\n",
    "    b1 = zeros(hidden_size1)\n",
    "    W2 = randn(hidden_size2, hidden_size1)\n",
    "    b2 = zeros(hidden_size2)\n",
    "    W3 = randn(output_size, hidden_size2)\n",
    "    b3 = zeros(output_size)\n",
    "    W4 = randn(output_size, hidden_size2)\n",
    "    b4 = zeros(output_size)\n",
    "    return NeuralNetwork(W1, b1, W2, b2, W3, b3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backpropagate! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Daten müssen irgendwie noch umgeändert werden, denn das funktioniert so nicht, da trainset nicht einfach nur \n",
    "# ein array oder int ist\n",
    "data = 1\n",
    "\n",
    "using Zygote\n",
    "\n",
    "function backpropagate!(model, data, y_true, learning_rate)\n",
    "    # Forward pass\n",
    "    # zi ist die Summe der gewichteten Eingangsdaten\n",
    "    # yi ist der hidden state, also die Aktivierungsfunktion auf die zi angewandt\n",
    "    z1 = NeuronalNetwork.W1 * data .+ NeuronalNetwork.b1\n",
    "    y1 = MaNN.leakyrelu(z1)\n",
    "    z2 = NeuronalNetwork.W2 * y1 .+ NeuronalNetwork.b2\n",
    "    y2 = MaNN.leakyrelu(z2)\n",
    "    z3 = NeuronalNetwork.W3 * y2 .+ NeuronalNetwork.b3\n",
    "    y3 = MaNN.leakyrelu(z3)\n",
    "    y4 = MaNN.softmax(y3)\n",
    "\n",
    "    # Compute loss\n",
    "    # a4 ist der Output des neuronalen Netzes\n",
    "    loss = MaNN.cross_entropy(y_true, y4)\n",
    "\n",
    "    # Backpropagation\n",
    "    ## unbedingt mit zygote(reversediff) schreiben\n",
    "    ableitungW4 = gradient(loss, W4)\n",
    "    W4 = NeuronalNetwork.W4 - learning_rate*ableitungW4\n",
    "    ableitungW3 = gradient(loss, W3)\n",
    "    W3 = NeuronalNetwork.W3 - learning_rate*ableitungW3\n",
    "    ableitungW2 = gradient(loss, W2)\n",
    "    W2 = NeuronalNetwork.W2 - learning_rate*ableitungW2\n",
    "    ableitungW1 = gradient(loss, W1)\n",
    "    W1 = NeuronalNetwork.W1 - learning_rate*ableitungW1\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_hardcoded(model, data, labels, learning_rate, num_epochs)\n",
    "    for epoch in 1:num_epochs\n",
    "        total_loss = 0\n",
    "        for i in 1:size(trainset, 2)\n",
    "            x = trainset[:,i]\n",
    "            y_true = labels[:,i]\n",
    "            loss = backporpagate!(model, x, y_true, learning_rate)\n",
    "            total_loss += loss\n",
    "        end\n",
    "        println(\"Epoch:\" +epoch+ \"Loss:\" +total_loss)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: space required before colon in \"?\" expression",
     "output_type": "error",
     "traceback": [
      "syntax: space required before colon in \"?\" expression\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Luisa\\Documents\\Seminar Computational Statistics\\MaNN.jl\\src\\autodifferentiation.ipynb:5"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "labels = [i == j ? 1.0:0.0 for i in 1:output_size, j in 1:100]\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "train_hardcoded(model, data, labels, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
