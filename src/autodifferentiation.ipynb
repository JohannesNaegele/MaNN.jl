{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [27, 81]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       "  18\n",
       " 108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function f!(y,x)\n",
    "    y[1] = 3*x^2\n",
    "    y[2] = x^4\n",
    "    nothing\n",
    "end\n",
    "\n",
    "y = [0,0]\n",
    "f!(y,3)\n",
    "@show y\n",
    "\n",
    "y = [0,0]\n",
    "ForwardDiff.derivative(f!, y, 3) #Syntax .derivative(funktion, Variable, Stelle der Ableitung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       "  6\n",
       " 32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g(x) = 3*x[1]^2+x[2]^4\n",
    "g([1,2])\n",
    "ForwardDiff.gradient(g,  [1,2])\n",
    "\n",
    "h(x,p) = p[1] *x[1]^2 +p[2]*x[2]^4\n",
    "h([1,2], [3,1])\n",
    "ForwardDiff.gradient(x -> h(x, [3,1]),[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x = 2.0: (3.5838531634528574,)\n"
     ]
    }
   ],
   "source": [
    "using ReverseDiff\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = x^2 + sin(x)\n",
    "\n",
    "# Define the input value\n",
    "x = 2.0\n",
    "\n",
    "# Compute the gradient\n",
    "gradient_result = gradient(f, x)\n",
    "\n",
    "println(\"Gradient at x = $x: $gradient_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x = 2.0: 3.5838531634528574\n"
     ]
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = x^2 + sin(x)\n",
    "\n",
    "# Define the input value\n",
    "x = 2.0\n",
    "\n",
    "# Compute the gradient\n",
    "gradient_result = ForwardDiff.derivative(f, x)\n",
    "\n",
    "println(\"Gradient at x = $x: $gradient_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVERSEDIFF FASTER THAN FORWARDDIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.175 μs (18 allocations: 16.28 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.076 ms (173 allocations: 8.44 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000-element Vector{Float64}:\n",
       " 1.4404907492776222\n",
       " 0.3641136609090274\n",
       " 0.6578963057858984\n",
       " 0.860395528110609\n",
       " 1.2044421778501537\n",
       " 1.2142689380070693\n",
       " 0.3290150644767269\n",
       " 1.278188466856067\n",
       " 1.7407254585747634\n",
       " 1.9545773140781482\n",
       " ⋮\n",
       " 0.5680727783008848\n",
       " 1.706572468315645\n",
       " 0.66622831666345\n",
       " 1.19005750345353\n",
       " 1.0382907953611165\n",
       " 1.0958346723238006\n",
       " 0.16621766339843091\n",
       " 0.5207973292796837\n",
       " 0.5442212367900865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ReverseDiff\n",
    "using BenchmarkTools\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = sum(x .^ 2)\n",
    "\n",
    "# Input values\n",
    "x = rand(1000)\n",
    "\n",
    "# Benchmark ReverseDiff (backward differentiation)\n",
    "@btime gradient(f, x)\n",
    "\n",
    "# Benchmark ForwardDiff (forward differentiation)\n",
    "using ForwardDiff\n",
    "@btime ForwardDiff.gradient(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "µs: Microsekunden 1µs = 1*10^-6 s\n",
    "ms: Millisekunden 1ms = 1*10^-3 s\n",
    "ReverseDiff: 7*10^-6s = 0,000007s\n",
    "ForwardDiff: 2,049*10^-3s = 0,002049s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.300 ns (0 allocations: 0 bytes)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  5.300 ns (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ReverseDiff\n",
    "using BenchmarkTools\n",
    "\n",
    "# Define a simple function\n",
    "f(x) = sum(x .^ 2)\n",
    "\n",
    "# Input values\n",
    "x = [1,2,3,4,5]\n",
    "\n",
    "# Benchmark ReverseDiff (backward differentiation)\n",
    "ableitungsrev = gradient(f,x)\n",
    "@btime ableitungsrev\n",
    "\n",
    "\n",
    "# Benchmark ForwardDiff (forward differentiation)\n",
    "using ForwardDiff\n",
    "ableitungsforw = ForwardDiff.gradient(f, x)\n",
    "@btime ableitungsforw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVERSE MODE VS. FORWARD MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input-Dimension: Vektor\n",
    "Output-Dimension: Skalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.911 μs (25 allocations: 1.94 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  800.000 ns (5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " allocations: 784 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       "  0.7071067811865476\n",
       " 12.0\n",
       "  8.0\n",
       "  6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "f(x::Vector) = sin(x[1]) + prod(x[2:end]);\n",
    "x = vcat(pi/4, 2:4)\n",
    "f(x)\n",
    "\n",
    "using Zygote\n",
    "@btime reversemode = gradient(f,x)\n",
    "\n",
    "using ForwardDiff\n",
    "@btime forwardmode = ForwardDiff.gradient(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Fall ist reverse mode marginal langsamer als forward mode, denn \n",
    "Input-Dimension = 2\n",
    "Output-Dimension = 1\n",
    "Es lässt sich erst ein deutlich größerer Unterschied feststellen, wenn man eine komplexere Funktion nimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reversemode == forwardmode\n",
    "# ergibt false, da forwardmode ist ein Vektor und reversemode ist ein Tupel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input-Dimension: Vektor mit 200 Einträgen\n",
    "Output-Dimension: Skalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.275 μs (25 allocations: 8.66 KiB)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  111.800 μs (39 allocations: 369.45 KiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "f(x) = sum(x .^ 2);\n",
    "x = rand(200);\n",
    "\n",
    "using Zygote\n",
    "@btime gradient(f,x);\n",
    "\n",
    "using ForwardDiff\n",
    "@btime ForwardDiff.gradient(f, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reverse mode schneller als forward mode\n",
    "Input-Dimension=200 >> Ouput-Dimension=1\n",
    "reverse mode: 1,68*10^-6s = 0,00000168s\n",
    "forward mode: 58*10^-6s = 0,000058s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000515 seconds (428 allocations: 1.680 MiB)\n",
      "  0.050726 seconds (623 allocations: 75.508 MiB, 28.18% gc time)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using Zygote\n",
    "@time hessian(f,x)\n",
    "\n",
    "using ForwardDiff\n",
    "@time ForwardDiff.hessian(f,x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input-Dimension: Vektor\n",
    "Output-Dimension: Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Berechnung der Jacobi-Matrix mittels reverse diff dauert"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  246.000 μs (817 allocations: 39.16 KiB)\n",
      "Die Berechnung der Jacobi-Matrix mittels forward diff dauert"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.990 μs (14 allocations: 1.28 KiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "f(x) = [[(x[1]^2+sin(x[2])), (exp(x[2])+3), x[1]*x[2]] [(2*x[1]^4*x[2]), exp(x[1]), (x[1]+x[2])] [sin(x[1]), 5x[1]+x[2]^2, 1]]\n",
    "x = [1 2]\n",
    "f(x)\n",
    "\n",
    "using Zygote\n",
    "print(\"Die Berechnung der Jacobi-Matrix mittels reverse diff dauert\")\n",
    "@btime jacobian(f,x)\n",
    "\n",
    "using ForwardDiff\n",
    "print(\"Die Berechnung der Jacobi-Matrix mittels forward diff dauert\")\n",
    "@btime ForwardDiff.jacobian(f, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input-Dimension: 2x1 Vektor\n",
    "Ouput-Dimension: 3x3 Matrix\n",
    "Input-Dimension < Output-Dimension\n",
    "Berechnung Jacobi-Matrix\n",
    "reverse mode: 164,4*10^-6s = 0,0001644s\n",
    "forward mode: 771,698*10^-9s = 0,000000771698s\n",
    "-> forward mode ist deutlich schnller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTODIFF: BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTODIFF IN LAYERS\n",
    "Trainieren des neuronalen Netzes, indem die Gewichte verbessert werden. Das geschieht durch Anwendung des Gradientenverfahrens, denn es muss das Minimum der Funktion E gefunden werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [0,1,2,3,4,5,6,7,8,9]\n",
    "ouput = [0,1,2,3,4,5,6,7,8,9]\n",
    "gewichtsmatrix = randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MaNN\n",
    "# Struktur des neuronalen Netzes\n",
    "# implementiere jeweils 4 Gewichts- und 4 Biasmatrizen\n",
    "mutable struct NeuronalNetwork\n",
    "    W1::Matrix{Float64}\n",
    "    b1::Vector{Float64}\n",
    "    W2::Matrix{Float64}\n",
    "    b2::Vector{Float64}\n",
    "    W3::Matrix{Float64}\n",
    "    b3::Vector{Float64}\n",
    "    W4::Matrix{Float64}\n",
    "    b4::Vector{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize_network (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funktion, die die Gewichts- und Biasmatrizen in ihrer jeweiligen Größe mit zufälligen Werten initialisiert\n",
    "# W hat so viele Zeilen wie Neuronen im nächsten hidden layer sind und so viele Spalten wie die Anzahl der Inputs ist\n",
    "# b ist ein Vektor mit so vielen Zeilen wie Neuronen in dem nächsten hidden layer sind\n",
    "function initialize_network(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    W1 = randn(hidden_size1, input_size)\n",
    "    b1 = zeros(hidden_size1)\n",
    "    W2 = randn(hidden_size2, hidden_size1)\n",
    "    b2 = zeros(hidden_size2)\n",
    "    W3 = randn(output_size, hidden_size2)\n",
    "    b3 = zeros(output_size)\n",
    "    W4 = randn(output_size, hidden_size2)\n",
    "    b4 = zeros(output_size)\n",
    "    return NeuralNetwork(W1, b1, W2, b2, W3, b3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backpropagate! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Daten müssen irgendwie noch umgeändert werden, denn das funktioniert so nicht, da trainset nicht einfach nur \n",
    "# ein array oder int ist\n",
    "data = 1\n",
    "\n",
    "using Zygote\n",
    "\n",
    "function backpropagate!(model, data, y_true, learning_rate)\n",
    "    # Forward pass\n",
    "    # zi ist die Summe der gewichteten Eingangsdaten\n",
    "    # yi ist der hidden state, also die Aktivierungsfunktion auf die zi angewandt\n",
    "    z1 = NeuronalNetwork.W1 * data .+ NeuronalNetwork.b1\n",
    "    y1 = MaNN.leakyrelu(z1)\n",
    "    z2 = NeuronalNetwork.W2 * y1 .+ NeuronalNetwork.b2\n",
    "    y2 = MaNN.leakyrelu(z2)\n",
    "    z3 = NeuronalNetwork.W3 * y2 .+ NeuronalNetwork.b3\n",
    "    y3 = MaNN.leakyrelu(z3)\n",
    "    y4 = MaNN.softmax(y3)\n",
    "\n",
    "    # Compute loss\n",
    "    # a4 ist der Output des neuronalen Netzes\n",
    "    loss = MaNN.cross_entropy(y_true, y4)\n",
    "\n",
    "    # Backpropagation\n",
    "    ## unbedingt mit zygote(reversediff) schreiben\n",
    "    ableitungW4 = gradient(loss, W4)\n",
    "    W4 = NeuronalNetwork.W4 - learning_rate*ableitungW4\n",
    "    ableitungW3 = gradient(loss, W3)\n",
    "    W3 = NeuronalNetwork.W3 - learning_rate*ableitungW3\n",
    "    ableitungW2 = gradient(loss, W2)\n",
    "    W2 = NeuronalNetwork.W2 - learning_rate*ableitungW2\n",
    "    ableitungW1 = gradient(loss, W1)\n",
    "    W1 = NeuronalNetwork.W1 - learning_rate*ableitungW1\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_hardcoded(model, data, labels, learning_rate, num_epochs)\n",
    "    for epoch in 1:num_epochs\n",
    "        total_loss = 0\n",
    "        for i in 1:size(trainset, 2)\n",
    "            x = trainset[:,i]\n",
    "            y_true = labels[:,i]\n",
    "            loss = backporpagate!(model, x, y_true, learning_rate)\n",
    "            total_loss += loss\n",
    "        end\n",
    "        println(\"Epoch:\" +epoch+ \"Loss:\" +total_loss)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: space required before colon in \"?\" expression",
     "output_type": "error",
     "traceback": [
      "syntax: space required before colon in \"?\" expression\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Luisa\\Documents\\Seminar Computational Statistics\\MaNN.jl\\src\\autodifferentiation.ipynb:5"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "labels = [i == j ? 1.0:0.0 for i in 1:output_size, j in 1:100]\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "train_hardcoded(model, data, labels, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementiere jeweils 3 Gewichts- und 3 Biasmatrizen\n",
    "# Define the neural network architecture\n",
    "using Zygote\n",
    "\n",
    "function backpropagate!(model, data, y_true, learning_rate)\n",
    "    for data in trainset\n",
    "        data = trainset.features\n",
    "        ableitung = gradient(model) do m\n",
    "            result = m(input)\n",
    "            loss = MaNN.cross_entropy(y_true, result)\n",
    "            return loss\n",
    "            print(loss)\n",
    "        end\n",
    "        #Gewichte updaten\n",
    "        Dense.weights -= learning_rate*ableitung[weights]\n",
    "        Dense.biases -= learning_rate*ableitung[biases]\n",
    "    end\n",
    "end\n",
    "\n",
    "backpropagate!(model, trainset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "x = [2, 1];\n",
    "y = [2, 0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Output should be scalar; gradients are not defined for output var\"#f#18\"()",
     "output_type": "error",
     "traceback": [
      "Output should be scalar; gradients are not defined for output var\"#f#18\"()\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base .\\error.jl:35\n",
      " [2] sensitivity(y::Function)\n",
      "   @ Zygote C:\\Users\\Luisa\\.julia\\packages\\Zygote\\gsq4u\\src\\compiler\\interface.jl:68\n",
      " [3] gradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "   @ Zygote C:\\Users\\Luisa\\.julia\\packages\\Zygote\\gsq4u\\src\\compiler\\interface.jl:97\n",
      " [4] top-level scope\n",
      "   @ c:\\Users\\Luisa\\Documents\\Seminar Computational Statistics\\MaNN.jl\\src\\autodifferentiation.ipynb:1"
     ]
    }
   ],
   "source": [
    "gs = gradient(Flux.params(x, y)) do\n",
    "    f(x, y)\n",
    "  end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grads(...)\n",
    "gs[x]\n",
    "\n",
    "gs[y]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
